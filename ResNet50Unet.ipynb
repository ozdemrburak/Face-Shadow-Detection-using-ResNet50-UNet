{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfW4d3lRZ0GG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1uN05tsGZ6eI"
   },
   "outputs": [],
   "source": [
    "class image_shadow_dataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform = None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.images[index]\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        base_name = os.path.splitext(img_name)[0]\n",
    "        mask_name = base_name + '_shadow.png'\n",
    "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype = np.float32)\n",
    "        mask[mask == 255.0] = 1.0\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image = image, mask = mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dv520ov-aLcn",
    "outputId": "ea501183-4cd5-41e4-d656-c013381e5299"
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "transform_n = A.Compose([ \n",
    "    A.Normalize(mean=[0.36184, 0.34747, 0.33529],         #calculation of normalize values down below\n",
    "                        std=[0.22139, 0.22265, 0.24461]), #https://forums.fast.ai/t/image-normalization-in-pytorch/7534/7?u=laochanlam\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "T6Xu-9wMaTJm",
    "outputId": "177f5d00-1968-435f-845d-e56978a0b179"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "train_ds = image_shadow_dataset(image_dir = train_dir,\n",
    "                                    mask_dir= train_masks,\n",
    "                                    transform = transform_n)\n",
    "train_data_loader = DataLoader(dataset = train_ds,\n",
    "                               batch_size= BATCH_SIZE,\n",
    "                               num_workers= 0,\n",
    "                               shuffle = True)\n",
    "val_ds = image_shadow_dataset(image_dir = val_dir,\n",
    "                                   mask_dir = val_masks,\n",
    "                                   transform = transform_n)\n",
    "val_data_loader = DataLoader(dataset = val_ds,\n",
    "                            batch_size = BATCH_SIZE,\n",
    "                            num_workers = 0,\n",
    "                            shuffle= False)\n",
    "test_ds = image_shadow_dataset(image_dir = test_dir,\n",
    "                                    mask_dir= test_masks,\n",
    "                                    transform = transform_n)\n",
    "test_data_loader = DataLoader(dataset = test_ds,\n",
    "                              batch_size = BATCH_SIZE,\n",
    "                              num_workers = 0,\n",
    "                              shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Model Classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NysLbpOKaiii"
   },
   "outputs": [],
   "source": [
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, backbone = 'resnet50', pretrained = True, channels = [64, 256, 512, 1024, 2048]):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "        resnet = models.resnet50(pretrained = True)\n",
    "        self.channels = channels\n",
    "        self.stem = nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "        )\n",
    "        self.maxpool = resnet.maxpool\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        self.encoder_layers = nn.ModuleList([self.layer1,\n",
    "                                             self.layer2,\n",
    "                                             self.layer3,\n",
    "                                             self.layer4])\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        x = self.stem(x)\n",
    "        skip_connections.append(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        for layers in self.encoder_layers:\n",
    "            x= layers(x)\n",
    "            skip_connections.append(x)\n",
    "\n",
    "        return skip_connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQuAZcBEaWRX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oG09fdDWaY8D"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_channels = 1, encoder_channels = [64, 256, 512, 1024, 2048],\n",
    "                                         decoder_channels = [1024, 512, 256, 128, 64]):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.ModuleList()\n",
    "        self.final_conv = nn.Sequential(nn.ConvTranspose2d(in_channels =64,\n",
    "                                         out_channels=64,\n",
    "                                         kernel_size = 2,\n",
    "                                         stride = 2),\n",
    "                                        nn.Conv2d(decoder_channels[-1], out_channels, kernel_size=1))\n",
    "        self.use_skip = [True, True,True,False, True]\n",
    "        for idx in range(len(decoder_channels)):\n",
    "            self.decoder.append(\n",
    "                nn.ConvTranspose2d(in_channels= decoder_channels[idx]*2,\n",
    "                                   out_channels= decoder_channels[idx],\n",
    "                                   kernel_size= 2,\n",
    "                                   stride = 2)\n",
    "            )\n",
    "            if self.use_skip[idx] == False:\n",
    "                self.decoder.append(nn.Sequential(nn.Conv2d(in_channels = 128,\n",
    "                                                                     out_channels = 256,\n",
    "                                                                     kernel_size = 1,\n",
    "                                                                     stride = 1),\n",
    "                                                  DoubleConv(in_channels=   encoder_channels[-(idx+1)],\n",
    "                                                             out_channels= decoder_channels[idx]),\n",
    "\n",
    "                                                 )\n",
    "                                   )\n",
    "            else:\n",
    "                self.decoder.append(DoubleConv(in_channels=  decoder_channels[idx]*2,\n",
    "                                               out_channels= decoder_channels[idx]))\n",
    "\n",
    "    def forward(self, skip_connections):\n",
    "        x = skip_connections[-1]\n",
    "        skip_connections_reversed = skip_connections[:-1][::-1]\n",
    "\n",
    "        for i in range(0, len(self.decoder), 2):\n",
    "            if self.use_skip[i//2]:\n",
    "                x = self.decoder[i](x)\n",
    "                skip = skip_connections_reversed[i//2]\n",
    "\n",
    "                if x.shape != skip.shape:\n",
    "                    x = TF.resize(x, size=skip.shape[2:])\n",
    "                concat_skip = torch.cat((skip, x), dim=1)\n",
    "                x = self.decoder[i+1](concat_skip)\n",
    "            else:\n",
    "                x = self.decoder[i](x) #up sampling\n",
    "                x = self.decoder[i+1](x)\n",
    "                skip_connections_reversed.insert(i//2, skip_connections_reversed[i//2])\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9e3_Imdvaa7E"
   },
   "outputs": [],
   "source": [
    "class ResNet50UNet(nn.Module):\n",
    "    def __init__(self, encoder: nn.Module,\n",
    "                       decoder: nn.Module):\n",
    "        super(ResNet50UNet, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self, x):\n",
    "        encoder_out = self.encoder(x)\n",
    "        decoder_out = self.decoder(encoder_out)\n",
    "\n",
    "        return decoder_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rl-GkEkGagW8",
    "outputId": "e090210f-4e02-4bbe-fded-b123c78903c4"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "x = torch.randn((3, 3, 512, 512))\n",
    "model = ResNet50UNet(encoder = ResNetEncoder(),\n",
    "                         decoder = Decoder())\n",
    "preds = model(x)\n",
    "assert preds.shape[2:] == x.shape[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "bNlBF1zxa0Ku",
    "outputId": "2aee778b-a39d-438e-b922-3cd3cf9c8162"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JR_9_UR_dL9c"
   },
   "outputs": [],
   "source": [
    "from surface_distance import metrics\n",
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               scaler: torch.cuda.amp.GradScaler,\n",
    "               device: torch.device):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_dice = 0\n",
    "    train_iou = 0\n",
    "    train_surface_dice = 0\n",
    "    total_pixels = 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        X = X.float()\n",
    "        y = y.float()\n",
    "        with torch.cuda.amp.autocast():\n",
    "          y_pred = model(X)\n",
    "          loss = loss_fn(y_pred, y.unsqueeze(1))\n",
    "          train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        #get predicted class (0 or 1)\n",
    "        #y_pred_class = torch.argmax(y_pred, dim=1)\n",
    "        y_pred_class = (torch.sigmoid(y_pred) > 0.5).float().squeeze(1)\n",
    "\n",
    "        #dice score\n",
    "        intersect = (y_pred_class * y).sum().item()\n",
    "        dice = (2.0 * intersect) / (y_pred_class.sum().item() + y.sum().item() + 1e-8)\n",
    "        train_dice += dice\n",
    "        #IoU score\n",
    "        union = y_pred_class.sum().item() + y.sum().item() - intersect\n",
    "        iou = intersect / (union + 1e-8)\n",
    "        train_iou += iou\n",
    "\n",
    "        #surface_dice\n",
    "        y_np= y.cpu().numpy()\n",
    "        y_pred_np = y_pred_class.cpu().numpy()\n",
    "        for i in range(y_np.shape[0]):\n",
    "          surface_distances = metrics.compute_surface_distances(\n",
    "            y_np[i] > 0,\n",
    "            y_pred_np[i] > 0,\n",
    "            spacing_mm=(1.0, 1.0)\n",
    "        )\n",
    "          surface_dice = metrics.compute_surface_dice_at_tolerance(\n",
    "                surface_distances,\n",
    "                tolerance_mm=7.0,\n",
    "            )\n",
    "          train_surface_dice += surface_dice\n",
    "\n",
    "        #accuracy\n",
    "        correct_pixels = (y_pred_class == y).sum().item()\n",
    "        batch_pixels = y.numel() #numel returns len of y -> total pixels of the batch\n",
    "        total_pixels += batch_pixels\n",
    "        train_acc += correct_pixels\n",
    "\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / total_pixels\n",
    "    train_dice = train_dice / len(dataloader)\n",
    "    train_iou = train_iou / len(dataloader)\n",
    "    train_surface_dice /= (len(dataloader) * dataloader.batch_size)\n",
    "    return train_loss, train_acc, train_dice, train_surface_dice, train_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTmNAFQldM5c"
   },
   "outputs": [],
   "source": [
    "def validation_step(model: torch.nn.Module,\n",
    "                    dataloader: torch.utils.data.DataLoader,\n",
    "                    loss_fn = torch.nn.Module,\n",
    "                    device = torch.device):\n",
    "    model.eval()\n",
    "    val_loss, val_acc = 0, 0\n",
    "    total_pixels = 0\n",
    "    val_dice = 0\n",
    "    val_iou = 0\n",
    "    val_surface_dice = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            X = X.float()\n",
    "            y = y.float()\n",
    "            val_pred_logits = model(X)\n",
    "            loss = loss_fn(val_pred_logits, y.unsqueeze(1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            #val_pred_labels = torch.argmax(val_pred_logits, dim = 1)\n",
    "            val_pred_labels = (torch.sigmoid(val_pred_logits) > 0.5).float().squeeze(1)\n",
    "            #dice\n",
    "            intersect = (val_pred_labels * y).sum().item()\n",
    "            dice = (2.0 * intersect) / (val_pred_labels.sum().item() + y.sum().item() + 1e-8)\n",
    "            val_dice += dice\n",
    "            #iou\n",
    "            union = val_pred_labels.sum().item() + y.sum().item() - intersect\n",
    "            iou = intersect / (union + 1e-8)\n",
    "            val_iou += iou\n",
    "\n",
    "            #surface dice\n",
    "            y_np= y.cpu().numpy()\n",
    "            val_pred_labels_np = val_pred_labels.cpu().numpy()\n",
    "            for i in range(y_np.shape[0]):\n",
    "              surface_distances = metrics.compute_surface_distances(\n",
    "                  y_np[i] > 0,\n",
    "                  val_pred_labels_np[i] > 0,\n",
    "                  spacing_mm=(1.0, 1.0)\n",
    "        )\n",
    "              surface_dice = metrics.compute_surface_dice_at_tolerance(\n",
    "                surface_distances,\n",
    "                tolerance_mm=7.0,\n",
    "            )\n",
    "              val_surface_dice += surface_dice\n",
    "\n",
    "            #accuracy\n",
    "            correct_pixels = (val_pred_labels == y).sum().item()\n",
    "            batch_pixels = y.numel()\n",
    "            total_pixels += batch_pixels\n",
    "            val_acc += correct_pixels\n",
    "\n",
    "        val_dice = val_dice / len(dataloader)\n",
    "        val_loss = val_loss / len(dataloader)\n",
    "        val_acc = val_acc / total_pixels if total_pixels > 0 else 0\n",
    "        val_surface_dice /= (len(dataloader) * dataloader.batch_size)\n",
    "        val_iou = val_iou / len(dataloader)\n",
    "    return val_loss, val_acc, val_dice,val_iou, val_surface_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-R0tFsehe-Pg"
   },
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module,\n",
    "                    dataloader: torch.utils.data.DataLoader,\n",
    "                    loss_fn = torch.nn.Module,\n",
    "                    device = torch.device):\n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    total_pixels = 0\n",
    "    test_dice = 0\n",
    "    test_iou = 0\n",
    "    test_surface_dice = 0\n",
    "    with torch.inference_mode():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            X = X.float()\n",
    "            y = y.float()\n",
    "            test_pred_logits = model(X)\n",
    "            loss = loss_fn(test_pred_logits, y.unsqueeze(1))\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            #test_pred_labels = torch.argmax(test_pred_logits, dim = 1)\n",
    "            test_pred_labels = (torch.sigmoid(test_pred_logits) > 0.5).float().squeeze(1)\n",
    "            #dice\n",
    "            intersect = (test_pred_labels * y).sum().item()\n",
    "            dice = (2.0 * intersect) / (test_pred_labels.sum().item() + y.sum().item() + 1e-8)\n",
    "            test_dice += dice\n",
    "            #iou score\n",
    "            union = test_pred_labels.sum().item() + y.sum().item() - intersect\n",
    "            iou = intersect / (union + 1e-8)\n",
    "            test_iou += iou\n",
    "\n",
    "            #surface_dice\n",
    "            y_np= y.cpu().numpy()\n",
    "            test_pred_labels_np = test_pred_labels.cpu().numpy()\n",
    "            for i in range(y_np.shape[0]):\n",
    "              surface_distances = metrics.compute_surface_distances(\n",
    "                y_np[i] > 0,\n",
    "                test_pred_labels_np[i] > 0,\n",
    "                spacing_mm=(1.0, 1.0)\n",
    "        )\n",
    "              surface_dice = metrics.compute_surface_dice_at_tolerance(\n",
    "                surface_distances,\n",
    "                tolerance_mm=7.0,\n",
    "            )\n",
    "\n",
    "              test_surface_dice += surface_dice\n",
    "\n",
    "\n",
    "            correct_pixels = (test_pred_labels == y).sum().item()\n",
    "            batch_pixels = y.numel()\n",
    "            total_pixels += batch_pixels\n",
    "            test_acc += correct_pixels\n",
    "\n",
    "        test_dice = test_dice / len(dataloader)\n",
    "        test_loss = test_loss / len(dataloader)\n",
    "        test_acc = test_acc / total_pixels if total_pixels > 0 else 0\n",
    "        test_surface_dice /= (len(dataloader) * dataloader.batch_size)\n",
    "        test_iou = test_iou / len(dataloader)\n",
    "    return test_loss, test_acc, test_dice, test_iou, test_surface_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvCFlLRje_bZ"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "def train_loop(model: torch.nn.Module,\n",
    "               train_dataloader: torch.utils.data.DataLoader,\n",
    "               test_dataloader: torch.utils.data.DataLoader,\n",
    "               val_dataloader: torch.utils.data.DataLoader,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               epochs: int,\n",
    "               device: torch.device,\n",
    "               scaler: torch.cuda.amp.GradScaler = None,\n",
    "               scheduler: torch.optim.lr_scheduler._LRScheduler = None):\n",
    "\n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"train_dice\": [],\n",
    "               \"train_iou\": [],\n",
    "               \"train_surface_dice\": [],\n",
    "               \"val_loss\": [],\n",
    "               \"val_acc\": [],\n",
    "               \"val_dice\": [],\n",
    "               \"val_iou\": [],\n",
    "               \"val_surface_dice\": [],\n",
    "               \"test_loss\": [],\n",
    "               \"test_acc\": [],\n",
    "               \"test_dice\": [],\n",
    "               \"test_iou\": [],\n",
    "               \"test_surface_dice\": []\n",
    "               }\n",
    "    best_model_dice = 0.0\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc, train_dice,train_iou, train_surface_dice = train_step(model= model,\n",
    "                                           dataloader = train_dataloader,\n",
    "                                           loss_fn= loss_fn,\n",
    "                                           optimizer = optimizer,\n",
    "                                           scaler= scaler,\n",
    "                                           device = device)\n",
    "        val_loss, val_acc, val_dice,val_iou, val_surface_dice = validation_step(model = model,\n",
    "                                     dataloader = val_dataloader,\n",
    "                                     loss_fn = loss_fn,\n",
    "                                     device = device)\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"train_dice: {train_dice:.4f} | \"\n",
    "            f\"train_iou: {train_iou:.4f} | \"\n",
    "            f\"train_surface_dice: {train_surface_dice:.4f}\\n\"\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"val_loss: {val_loss:.4f} | \"\n",
    "            f\"val_acc: {val_acc:.4f} | \"\n",
    "            f\"val_dice: {val_dice:.4f} | \"\n",
    "            f\"val_iou: {val_iou:.4f} | \"\n",
    "            f\"val_surface_dice: {val_surface_dice:.4f}\\n\\n\"\n",
    "        )\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"train_dice\"].append(train_dice)\n",
    "        results[\"train_iou\"].append(train_iou)\n",
    "        results[\"train_surface_dice\"].append(train_surface_dice)\n",
    "        results[\"val_loss\"].append(val_loss)\n",
    "        results[\"val_acc\"].append(val_acc)\n",
    "        results[\"val_dice\"].append(val_dice)\n",
    "        results[\"val_iou\"].append(val_iou)\n",
    "        results[\"val_surface_dice\"].append(val_surface_dice)\n",
    "\n",
    "        filename = f\"/content/scheduler_state_dicts_2/{epoch}_best_model_{val_dice:.4f}dice_score.pth\"\n",
    "        if val_dice > best_model_dice:\n",
    "            best_model_dice = val_dice\n",
    "            torch.save(model.state_dict(), filename)\n",
    "            print(f\"New best model with validation dice: {val_dice:.2f}\")\n",
    "\n",
    "        if scheduler is not None:\n",
    "          scheduler.step()\n",
    "\n",
    "    test_loss, test_acc, test_dice,test_iou, test_surface_dice = test_step(model = model,\n",
    "                                    dataloader= test_dataloader,\n",
    "                                    loss_fn = loss_fn,\n",
    "                                    device = device)\n",
    "    print(f\"test_loss = {test_loss:.4f} | \"\n",
    "          f\"test_acc = {test_acc:.4f} |\"\n",
    "          f\"test_dice = {test_dice:.4f} |\"\n",
    "          f\"test_iou = {test_iou:4f} |\"\n",
    "          f\"test_surface_dice = {test_surface_dice:.4f}\"\n",
    "          )\n",
    "    results[\"test_loss\"] = test_loss\n",
    "    results[\"test_acc\"] = test_acc\n",
    "    results[\"test_dice\"] = test_dice\n",
    "    results[\"test_iou\"] = test_iou\n",
    "    results[\"test_surface_dice\"] = test_surface_dice\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmD3ATmdfEGJ"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "26d14e566059488695f01e94491ddf34",
      "ca3abd3679aa42cca3c48a61ee2b69e4",
      "dc64161bcca1458f8756da95d0b3a3ba",
      "3d5d3a920e9c4beabf9cc861ba14b1c3",
      "ddc463a2d5024f54951c128ac739fd17",
      "4faeb11f9b5743a09dda9819fa43c051",
      "c49149621f6a4a8e963743dc1dbc967d",
      "cffbde5edf0c450b88ee7caffa7ec8d0",
      "e898b238c02a48d1aca386374012841c",
      "6ac75fdd83d146e7b5ac2209febbef44",
      "7839474c06c144eda03d7e10c00740d8"
     ]
    },
    "id": "ULQd8j5hfF6M",
    "outputId": "651a2f67-7ade-4d27-c432-d265aa8ddffc"
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "from torch.optim import optimizer\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchvision.models as models\n",
    "NUM_EPOCHS = 20\n",
    "model = ResNet50UNet(encoder = ResNetEncoder(),\n",
    "                     decoder= Decoder()).to(device)\n",
    "#model.load_state_dict(torch.load(\"/content/scheduler_state_dicts/15epoch_best_model_0.8932dice_score.pth\"))\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = 0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "start_time = timer()\n",
    "model_0_results = train_loop(model= model,\n",
    "                       train_dataloader= train_data_loader,\n",
    "                       val_dataloader= val_data_loader,\n",
    "                       test_dataloader= test_data_loader,\n",
    "                       optimizer = optimizer,\n",
    "                       scaler = scaler,\n",
    "                       loss_fn = loss_fn,\n",
    "                       epochs = NUM_EPOCHS,\n",
    "                       device = device,\n",
    "                      scheduler = scheduler)\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7JWyA2SoJZCH"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"training_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_0_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rIaxDea0LSu5",
    "outputId": "e7fccace-a498-4238-ce43-ab13f05a405c"
   },
   "outputs": [],
   "source": [
    "results[\"val_loss\"]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "26d14e566059488695f01e94491ddf34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ca3abd3679aa42cca3c48a61ee2b69e4",
       "IPY_MODEL_dc64161bcca1458f8756da95d0b3a3ba",
       "IPY_MODEL_3d5d3a920e9c4beabf9cc861ba14b1c3"
      ],
      "layout": "IPY_MODEL_ddc463a2d5024f54951c128ac739fd17"
     }
    },
    "3d5d3a920e9c4beabf9cc861ba14b1c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ac75fdd83d146e7b5ac2209febbef44",
      "placeholder": "​",
      "style": "IPY_MODEL_7839474c06c144eda03d7e10c00740d8",
      "value": " 8/20 [06:51&lt;10:14, 51.18s/it]"
     }
    },
    "4faeb11f9b5743a09dda9819fa43c051": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ac75fdd83d146e7b5ac2209febbef44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7839474c06c144eda03d7e10c00740d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c49149621f6a4a8e963743dc1dbc967d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca3abd3679aa42cca3c48a61ee2b69e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4faeb11f9b5743a09dda9819fa43c051",
      "placeholder": "​",
      "style": "IPY_MODEL_c49149621f6a4a8e963743dc1dbc967d",
      "value": " 40%"
     }
    },
    "cffbde5edf0c450b88ee7caffa7ec8d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc64161bcca1458f8756da95d0b3a3ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cffbde5edf0c450b88ee7caffa7ec8d0",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e898b238c02a48d1aca386374012841c",
      "value": 8
     }
    },
    "ddc463a2d5024f54951c128ac739fd17": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e898b238c02a48d1aca386374012841c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
